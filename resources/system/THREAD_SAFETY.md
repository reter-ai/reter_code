# Thread-Safe RETER Access Implementation

**Date:** 2025-11-10
**Version:** reter-logical-thinking v2.1.0
**Status:** âœ… Complete

## Overview

RETER is **NOT thread-safe**. The reter-logical-thinking MCP server now implements **per-instance locking** to ensure serialized access to each RETER instance, preventing race conditions and data corruption.

## Problem Statement

### RETER Thread Safety Issue

RETER's C++ implementation is not thread-safe:
- No internal locking mechanisms
- Concurrent operations can corrupt internal state
- RETE network modifications must be serialized
- Queries during modifications can cause crashes

### Multi-Instance Challenge

With multiple RETER instances, we need to:
- âœ… **Allow parallel operations on DIFFERENT instances**
- âœ… **Serialize operations on the SAME instance**
- âœ… **Prevent deadlocks**
- âœ… **Maintain performance**

## Solution: Per-Instance Asyncio Locks

### Architecture

```python
class LogicalThinkingServer:
    def __init__(self):
        # Store multiple RETER instances
        self.reter_instances: Dict[str, ReterWrapper] = {}

        # One lock per instance for thread-safe access
        self.instance_locks: Dict[str, asyncio.Lock] = {}
```

### Key Design Decisions

1. **Asyncio Locks** - Compatible with FastMCP's async architecture
2. **Per-Instance Locking** - Different instances can be accessed in parallel
3. **Automatic Lock Creation** - Lock created with each new instance
4. **Context Manager Usage** - `async with lock:` ensures proper release

## Implementation Details

### Lock Management

```python
def _get_or_create_instance(self, instance_name: str) -> ReterWrapper:
    """Create instance + lock together"""
    if instance_name not in self.reter_instances:
        print(f"ğŸ†• Creating new RETER instance: '{instance_name}'")
        self.reter_instances[instance_name] = ReterWrapper()
        self.instance_locks[instance_name] = asyncio.Lock()  # Create lock
    return self.reter_instances[instance_name]

def _get_instance_lock(self, instance_name: str) -> asyncio.Lock:
    """Get lock for specific instance"""
    self._get_or_create_instance(instance_name)  # Ensure lock exists
    return self.instance_locks[instance_name]
```

### Protected Operations

All 7 tools now use locks:

#### 1. logical_thinking
```python
async def logical_thinking(instance_name: str, ...):
    reter = self._get_or_create_instance(instance_name)
    lock = self._get_instance_lock(instance_name)

    # ... prepare thought ...

    if logic_operation:
        async with lock:  # ğŸ”’ Serialize RETER access
            await self._execute_logic_operation(reter, logical_thought, logic_operation)
```

#### 2. add_knowledge
```python
async def add_knowledge(instance_name: str, source: str, ...):
    reter = self._get_or_create_instance(instance_name)
    lock = self._get_instance_lock(instance_name)

    async with lock:  # ğŸ”’ Serialize RETER access
        if type == "ontology":
            result = await reter.add_ontology(source, source_id)
        elif type == "python":
            result = await reter.load_python_file(source)
```

#### 3. quick_query
```python
async def quick_query(instance_name: str, query: str, type: str):
    reter = self._get_or_create_instance(instance_name)
    lock = self._get_instance_lock(instance_name)

    async with lock:  # ğŸ”’ Serialize RETER access
        if type == "reql":
            result = await reter.reql_select(query)
        elif type == "dl":
            result = await reter.dl_query(query)
```

#### 4. forget_source
```python
async def forget_source(instance_name: str, source: str):
    reter = self._get_or_create_instance(instance_name)
    lock = self._get_instance_lock(instance_name)

    async with lock:  # ğŸ”’ Serialize RETER access
        result = await reter.forget_logics(source)
```

#### 5. save_state
```python
async def save_state(instance_name: str, filename: str):
    reter = self._get_or_create_instance(instance_name)
    lock = self._get_instance_lock(instance_name)

    async with lock:  # ğŸ”’ Serialize RETER access
        result = await reter.save_network(filename)
```

#### 6. load_state
```python
async def load_state(instance_name: str, filename: str):
    reter = self._get_or_create_instance(instance_name)
    lock = self._get_instance_lock(instance_name)

    async with lock:  # ğŸ”’ Serialize RETER access
        result = await reter.load_network(filename)
```

#### 7. check_consistency
```python
async def check_consistency(instance_name: str):
    reter = self._get_or_create_instance(instance_name)
    lock = self._get_instance_lock(instance_name)

    async with lock:  # ğŸ”’ Serialize RETER access
        result = await reter.check_consistency()
```

## Performance Characteristics

### âœ… Parallel Access to Different Instances

```python
# These run in parallel - different instances, different locks
async def task1():
    add_knowledge(instance_name="instance_a", source="...")

async def task2():
    add_knowledge(instance_name="instance_b", source="...")

# Tasks 1 and 2 can run concurrently!
await asyncio.gather(task1(), task2())
```

### â³ Serialized Access to Same Instance

```python
# These run sequentially - same instance, same lock
async def task1():
    add_knowledge(instance_name="main", source="...")  # Acquires lock

async def task2():
    quick_query(instance_name="main", query="...")     # Waits for lock

# Task 2 waits for task 1 to complete
await asyncio.gather(task1(), task2())
```

### Timing Example

```python
# Scenario: 2 instances, 4 operations
# Without locking: Potential data corruption
# With per-instance locking: Safe + efficient

Instance A:
  - Op1: add_knowledge (100ms) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  - Op2: quick_query (50ms)               â”‚ Serialized on lock_a
                                          â†“
  Total: 150ms

Instance B:
  - Op3: add_knowledge (100ms) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  - Op4: quick_query (50ms)               â”‚ Serialized on lock_b
                                          â†“
  Total: 150ms

Wall-clock time: 150ms (parallel execution of A and B!)
Without parallelism: 300ms (sequential execution)
```

## Safety Guarantees

### âœ… Race Condition Prevention

```python
# WITHOUT LOCKING (UNSAFE):
# Thread 1: reter.add_ontology("A is_a Thing")
# Thread 2: reter.add_ontology("B is_a Thing")  # Can corrupt RETE network!

# WITH LOCKING (SAFE):
async with lock:
    reter.add_ontology("A is_a Thing")   # Exclusive access
# Lock released

async with lock:
    reter.add_ontology("B is_a Thing")   # Waits for lock, then exclusive access
```

### âœ… Deadlock Prevention

**No Nested Locks:**
- Each operation acquires exactly one lock
- No operation holds multiple instance locks
- Lock is always released via context manager

**Lock Ordering:**
- Locks acquired by instance name (string)
- Deterministic ordering prevents circular waits

### âœ… Exception Safety

```python
async with lock:
    try:
        result = await reter.add_ontology(source)
    except Exception as e:
        # Lock automatically released even on exception
        raise e
```

The `async with` context manager ensures the lock is **always released**, even if an exception occurs.

## Testing

### Test 1: Basic Lock Acquisition

```python
import asyncio
import time

async def test_lock_acquisition():
    server = LogicalThinkingServer()

    # Create instance (creates lock)
    reter = server._get_or_create_instance("test")
    lock = server._get_instance_lock("test")

    # Verify lock exists
    assert isinstance(lock, asyncio.Lock)

    # Verify lock is not held
    assert not lock.locked()

    print("âœ… Lock creation test passed")

asyncio.run(test_lock_acquisition())
```

### Test 2: Serialized Access

```python
async def test_serialized_access():
    server = LogicalThinkingServer()
    results = []

    async def operation(name: str, delay: float):
        reter = server._get_or_create_instance("test")
        lock = server._get_instance_lock("test")

        async with lock:
            results.append(f"{name} start")
            await asyncio.sleep(delay)
            results.append(f"{name} end")

    # Run operations concurrently (but they'll serialize on lock)
    await asyncio.gather(
        operation("op1", 0.1),
        operation("op2", 0.1)
    )

    # Verify serialization: start/end pairs don't interleave
    assert results == ["op1 start", "op1 end", "op2 start", "op2 end"] or \
           results == ["op2 start", "op2 end", "op1 start", "op1 end"]

    print("âœ… Serialization test passed")

asyncio.run(test_serialized_access())
```

### Test 3: Parallel Access to Different Instances

```python
async def test_parallel_instances():
    server = LogicalThinkingServer()
    start_time = time.time()

    async def operation(instance: str, delay: float):
        reter = server._get_or_create_instance(instance)
        lock = server._get_instance_lock(instance)

        async with lock:
            await asyncio.sleep(delay)

    # Run operations on different instances
    await asyncio.gather(
        operation("instance_a", 0.5),
        operation("instance_b", 0.5)
    )

    elapsed = time.time() - start_time

    # Should complete in ~0.5s (parallel), not ~1.0s (sequential)
    assert elapsed < 0.7, f"Expected parallel execution, got {elapsed}s"

    print("âœ… Parallel execution test passed")

asyncio.run(test_parallel_instances())
```

## Migration Impact

### Breaking Change: NO

This is an **internal implementation detail**. The API remains unchanged.

```python
# v2.1.0 without locks (hypothetical)
add_knowledge(instance_name="main", source="...")

# v2.1.0 with locks (actual)
add_knowledge(instance_name="main", source="...")  # Same API!
```

### Performance Impact

**Negligible for Single-Threaded Use:**
- Lock acquisition overhead: ~1-2 microseconds
- No lock contention if operations are sequential

**Significant for Concurrent Use:**
- Prevents crashes and data corruption
- Enables safe parallel access to different instances
- Small serialization overhead on same instance

## Best Practices

### 1. Minimize Lock Hold Time

```python
# âŒ BAD: Hold lock during I/O
async with lock:
    result = await reter.add_ontology(source)
    await asyncio.sleep(5)  # Don't do this!
    return result

# âœ… GOOD: Release lock quickly
async with lock:
    result = await reter.add_ontology(source)
# Lock released here

await asyncio.sleep(5)  # Do this outside lock
return result
```

### 2. Use Different Instances for Parallel Work

```python
# âœ… GOOD: Parallel processing on different instances
async def analyze_projects():
    await asyncio.gather(
        add_knowledge(instance_name="project_a", source="a.py"),
        add_knowledge(instance_name="project_b", source="b.py"),
        add_knowledge(instance_name="project_c", source="c.py")
    )
    # All three run in parallel!
```

### 3. Batch Operations When Possible

```python
# âŒ LESS EFFICIENT: Many small operations
for fact in facts:
    add_knowledge(instance_name="main", source=fact)
    # Lock acquired/released 100 times

# âœ… MORE EFFICIENT: One large operation
batch = "\n".join(facts)
add_knowledge(instance_name="main", source=batch)
# Lock acquired/released once
```

## Troubleshooting

### Issue: Operations Seem Slow

**Cause:** Lock contention - multiple operations competing for same instance

**Solution:**
1. Use multiple instances for parallel work
2. Batch operations when possible
3. Profile to find bottlenecks

### Issue: Deadlock Suspected

**Unlikely:** Our implementation avoids deadlocks by design

**Diagnosis:**
- Check for infinite loops in RETER operations
- Verify no external locking code conflicts
- Review asyncio event loop health

### Issue: Lock Not Released

**Impossible:** `async with` guarantees lock release

**If suspected:**
- Check server logs for exceptions
- Verify proper async/await usage
- Restart server to clear state

## Implementation Checklist

- âœ… Import asyncio
- âœ… Add instance_locks Dict
- âœ… Create lock with each instance
- âœ… Add _get_instance_lock method
- âœ… Wrap logical_thinking RETER calls in lock
- âœ… Wrap add_knowledge RETER calls in lock
- âœ… Wrap quick_query RETER calls in lock
- âœ… Wrap forget_source RETER calls in lock
- âœ… Wrap save_state RETER calls in lock
- âœ… Wrap load_state RETER calls in lock
- âœ… Wrap check_consistency RETER calls in lock
- âœ… Syntax validation passed
- âœ… Documentation complete

## Technical Specifications

**Lock Type:** `asyncio.Lock`
**Lock Granularity:** Per RETER instance
**Lock Scope:** Within each tool function
**Lock Lifetime:** Same as RETER instance (until server restart)

**Concurrency Model:**
- Async/await with asyncio
- Cooperative multitasking
- No OS threads involved

**Memory Overhead:**
- Lock object: ~100 bytes per instance
- Negligible compared to RETER instance (~10-50 MB)

## Related Documentation

- **Multi-Instance Guide:** `MULTIPLE_INSTANCES.md`
- **Implementation Details:** `MULTI_INSTANCE_IMPLEMENTATION.md`
- **Server Source:** `src/logical_thinking_server/server.py`

---

**Version:** 2.1.0
**Thread Safety:** âœ… Implemented
**Deadlock Risk:** âœ… None
**Performance Impact:** âœ… Minimal (beneficial for concurrency)
